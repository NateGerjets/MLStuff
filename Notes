Combatting overfitting:
  High train accuracy and very low train loss,
  High test accuracy but loss over 40%
    Test Loss: 0.5897229455659521
    Test accuracy: 0.9196565232054376
    
    
  Huge issue, Unbalanced test and training classes. Must be fixed 
  or use a different dataset
  
  
  tanh better loss
  Test loss: 0.5593793627177563
  Test accuracy: 0.9127039717855405


  Tanh, 15 epochs, weighted classes
  Test loss: 0.44316784331599823
  Test accuracy: 0.9134044153920982
    
  Let it run for 30 epochs to see if test loss was helped by weighted classes
    Test loss: 0.5721660570326309
    Test accuracy: 0.9103951020939952

    Overfitting still a main issue
    
  10 epochs, no other changes
    Test loss: 0.38307232708515887
    Test accuracy: 0.9123667211564385
    
    
  Overfits very fast, have enough training examples, so use them
  Lower epochs will not allow enough time for overfitting
  
  5 epochs, no other changes
    Test loss: 0.393804121283649
    Test accuracy: 0.9006926608997322
    
    Not enough time to train
      Test loss: 0.36759124915112035
      Test accuracy: 0.9099281396844692
      
  Try a different approach. 10 epochs has had the best preformance, but try changing the 
  Dense core layer. 500 dims instead of a 1000
    Test loss: 0.3930424749905347
    Test accuracy: 0.9061405556775359
    
  Relu with 500 dimensions
    Test loss: 0.3656863158994306
    Test accuracy: 0.9133006459754905
    
  Maybe more epochs now (Lead to increase in loss but not in acc)
    Test loss: 0.4395993480967052
    Test accuracy: 0.9140789166472099
    
  With kernal size of 5 by 5
    Test loss: 0.41262221068045657
    Test accuracy: 0.901185565676167
    
  With stride of 1,1
    Test loss: 0.37390759623419023
    Test accuracy: 0.9333540872290251
  
  Try lower epochs, 5 exactly
    Test loss: 0.29010020053619096
    Test accuracy: 0.9307339092644624

    Test loss: 0.3291073570372817
  Test accuracy: 0.9308376786702461
  
  5 Epochs 6 by 6 kernel size
    Test loss: 0.278795008953818
    Test accuracy: 0.9338988766959815

    Test loss: 0.30048066954317065
    Test accuracy: 0.9312008716739885

  5 Epochs, Larger Conv2D Layers. (Too small for class size)
    Test loss: 0.27567406860113153
    Test accuracy: 0.9400990998110679

  More Epochs still leads to overfitting
  
  Try Underfitting then overfitting. First 2D at 32, second at 128
    Test loss: 0.3112153710586893
    Test accuracy: 0.9362855734557812
    
  (BEST) Try 128 dims for both 2D conv models (Sweet spot 5 epochs)
    val_loss: 0.2744 - 
    val_acc: 0.9425
    
    [0.8941277731168551, 0.9603511716485668, 0.9722290362148327, 0.9795881479571347, 0.9842360080046478]
[0.3954187649657876, 0.14020042532552623, 0.09412844129349372, 0.06822651024945743, 0.051797773997616745]
[0.9055179391500566, 0.9295924455781767, 0.933328144854385, 0.9381015383817158, 0.9383869043094724]
[0.3559342567307181, 0.27126432771991627, 0.27468616006534985, 0.26420992891720896, 0.29028651316164317]

[0.8864179200864764, 0.9578195941703955, 0.9716502054905339, 0.978562821423192, 0.9830492974415251]
[0.4273203382320612, 0.14826324709795297, 0.09701948311413383, 0.0697693579207564, 0.05413448950593414]
[0.9439244292397652, 0.9571794375241452, 0.9596324747702968, 0.9623437264661117, 0.9622576549837455]
[0.203509057172298, 0.15569171893679504, 0.1479589449074984, 0.14771006614770765, 0.151884711659845]

acc: 0.9828 - val_loss: 0.1516 - val_acc: 0.9627
              precision    recall  f1-score   support

           0       0.96      0.96      0.96      1000
           1       0.97      0.97      0.97      1000
           2       0.92      0.96      0.94      1000
           3       0.80      0.93      0.86       126
           4       0.96      0.93      0.94      1000
           5       0.88      0.92      0.90      1000
           6       0.95      0.93      0.94      1000
           7       0.89      0.90      0.89      1000
           8       0.89      0.94      0.91       767
           9       0.92      0.95      0.93      1000
          10       0.94      0.97      0.95      1000
          11       0.96      0.93      0.95      1000
          12       0.94      0.92      0.93      1000
          13       0.95      0.92      0.93       678
          14       0.98      0.79      0.87       629
          15       0.93      0.96      0.94      1000
          16       0.98      0.94      0.96       418
          17       0.95      0.95      0.95      1000
          18       0.96      0.92      0.94      1000
          19       0.95      0.96      0.95      1000
          20       0.92      0.90      0.91      1000
          21       0.89      0.95      0.92      1000
          22       0.93      0.90      0.91       336
          23       0.88      0.95      0.91       399
          24       0.93      0.95      0.94      1000
          25       0.95      0.90      0.92      1000
          26       0.98      0.95      0.97       836
          27       0.95      0.92      0.93      1000
          28       0.99      0.88      0.93      1000
          29       0.87      0.91      0.89       324
          30       0.88      0.96      0.92      1000
          31       0.96      0.90      0.93       498
          32       0.89      0.90      0.90       280
          33       0.93      0.95      0.94       552
          34       0.91      0.98      0.94      1000
          35       0.93      0.95      0.94      1000
          36       0.86      0.93      0.90       260
          37       0.98      0.97      0.97      1000
          38       0.92      0.93      0.93      1000
          39       0.96      0.90      0.93      1000
          40       0.93      0.89      0.91      1000
          41       0.95      0.92      0.94      1000
          42       0.98      0.93      0.95       348
          43       0.94      0.94      0.94       390
          44       0.91      0.71      0.79        68
          45       0.85      0.88      0.86        64
          46       0.95      0.95      0.95      1000
          47       0.95      0.99      0.97      1000
          48       0.83      0.91      0.87       574

   micro avg       0.93      0.93      0.93     38547
   macro avg       0.93      0.93      0.93     38547
weighted avg       0.93      0.93      0.93     38547

Test loss: 0.29443976579766845
Test accuracy: 0.9330687213012686
[0.8826845695324337, 0.9566092139496222, 0.9704021690013557, 0.9782400533591888, 0.9827857035242139]
[0.4397935988281104, 0.15300426540751086, 0.10214853318999674, 0.07295312930213611, 0.05571374513534798]
[0.9428485357089063, 0.9549630968519355, 0.9613539044189015, 0.9584705097596369, 0.9627310481354765]
[0.20448502759238302, 0.15736625917949906, 0.14235283310014277, 0.15654689834027494, 0.1515634368231834]


Testing best latin hyper params 

Test loss: 0.35003797024316713
Test accuracy: 0.927076037037953
[0.872259161237252, 0.949260861142616, 0.9625858025089837, 0.9711768123400855, 0.9772071955759258, 0.9814408366096783, 0.9846201019921414]
[0.47863178174886056, 0.17861659971862193, 0.12781788106656078, 0.0973618565930019, 0.07496671831971201, 0.06043133397662375, 0.048584778249248374]
[0.9368020140726874, 0.9451294299928906, 0.9527467561822909, 0.9584059561478623, 0.9578249736406085, 0.9568136337228068, 0.9589008671714674]
[0.2292726634586229, 0.19693276336049026, 0.1735567177567886, 0.1569772016807669, 0.16463559098881456, 0.17307561287097348, 0.17508915521260102]


Baseline 

Test loss: 0.3856551643600861
Test accuracy: 0.93524787921239
[0.8752609041757919, 0.9531609751898952, 0.9685731499985107, 0.976099025237897, 0.9818389172207518, 0.9856314419099432, 0.9877670905687174, 0.9899027392249263, 0.9908441460633056, 0.9921621156370366, 0.992775374948895, 0.9936629871107955]
[0.46585351909756023, 0.16338476428040916, 0.10745840090953278, 0.07904459741478383, 0.059128960070366575, 0.045853681224438496, 0.038491294383868016, 0.032139285750811535, 0.028720572570269325, 0.025246233695471423, 0.023301990856881553, 0.021530005943588816]
[0.940589159296796, 0.952079702192671, 0.9579971166053407, 0.9576958664170594, 0.9590514922643255, 0.961483011641168, 0.9629031911002087, 0.9619348869235901, 0.963979084629785, 0.9638284595356443, 0.9624513158177866, 0.9655498891829665]
[0.20929524994153895, 0.16817152838092442, 0.15232096964080688, 0.15730775418323242, 0.15384284861150935, 0.1615061385333336, 0.1632427919648503, 0.17422780406422123, 0.17195362368951844, 0.17719720597263622, 0.18843048251308134, 0.18082081037108366]


--------------------------------------- LATIN

  Same setup as hiragana
  Test loss: 0.19787374463348756
  Test accuracy: 0.9376442307692308
  [0.8864823717948718, 0.9373397435897436, 0.9488221153846154, 0.9558814102564103, 0.9615464743589743]
  [0.3590149961373745, 0.18251771605167633, 0.1443411741577662, 0.11747530317841431, 0.09883248169643756]
  [0.9241346153846154, 0.9308173076923076, 0.937548076923077, 0.9356730769230769, 0.9376442307692308]
  [0.22485569306291067, 0.2094613882096914, 0.1843337928618376, 0.1949463538796856, 0.19787374759880968]
  
Dims 64, 128, kernel_size 4,4 2,2 2,2 pool 3,3 2,2 strides 2,2 1,1
  Test loss: 0.18320452557531938
  Test accuracy: 0.9365865384615385
  [0.8679567307692307, 0.9290785256410257, 0.9390625, 0.9457211538461539, 0.9499439102564102]
  [0.42060469731306416, 0.2111528092928422, 0.17411184351413678, 0.15314625026323855, 0.13613986347730342]
  [0.9233173076923077, 0.9248557692307692, 0.9347596153846154, 0.9347596153846154, 0.9365865384615385]
  [0.23261537393698326, 0.22033262186325514, 0.19319173647090793, 0.19092894846716751, 0.18320452641409177]


  Test loss: 0.19961015021613845
Test accuracy: 0.9397596153846154
[0.8698157051282052, 0.9307051282051282, 0.9403044871794872, 0.9464903846153846, 0.9514022435897436, 0.9550560897435898, 0.9595032051282051, 0.9618589743589744, 0.9650641025641026, 0.9669070512820512]
[0.4149921686297808, 0.20686251657895552, 0.171791282108961, 0.15053390921690527, 0.1344298886641478, 0.1216457167153175, 0.10704454732246887, 0.09770789028933415, 0.08883207348294747, 0.08154055781184863]
[0.9183173076923077, 0.9339903846153846, 0.937451923076923, 0.9399038461538461, 0.9397596153846154, 0.9416826923076923, 0.9397115384615384, 0.9403846153846154, 0.9389903846153846, 0.9397596153846154]
[0.2471799156299004, 0.19891076885736905, 0.18861067918630747, 0.1855033911478061, 0.17771087331840626, 0.17916835471629522, 0.19418249360823, 0.18878367645755553, 0.20403982310269314, 0.1996101515346135]


precision    recall  f1-score   support

0       0.91      0.96      0.93       800
1       0.97      0.97      0.97       800
2       0.98      0.94      0.96       800
3       0.97      0.94      0.95       800
4       0.93      0.99      0.96       800
5       0.96      0.97      0.97       800
6       0.82      0.84      0.83       800
7       0.94      0.96      0.95       800
8       0.74      0.82      0.77       800
9       0.96      0.96      0.96       800
10       0.98      0.95      0.97       800
11       0.80      0.72      0.76       800
12       0.98      0.99      0.98       800
13       0.96      0.95      0.95       800
14       0.96      0.98      0.97       800
15       0.98      0.98      0.98       800
16       0.88      0.81      0.84       800
17       0.94      0.95      0.95       800
18       0.99      0.96      0.97       800
19       0.95      0.98      0.97       800
20       0.90      0.97      0.93       800
21       0.97      0.86      0.91       800
22       0.99      0.97      0.98       800
23       0.97      0.96      0.97       800
24       0.92      0.96      0.94       800
25       0.97      0.99      0.98       800

micro avg       0.94      0.94      0.94     20800
macro avg       0.94      0.94      0.94     20800
weighted avg       0.94      0.94      0.94     20800

Test loss: 0.20197419637883005
Test accuracy: 0.9352884615384616
[0.8539262820512821, 0.9233573717948718, 0.9360877403846154, 0.9431690705128205, 0.9484274839743589, 0.9526742788461539, 0.9566205929487179, 0.9603966346153846]
[0.4702955935245905, 0.22643940146916952, 0.18537251750150552, 0.16054284060135102, 0.14346833570549886, 0.12849793545901775, 0.11551570809947756, 0.1033958765439307]
[0.9175480769230769, 0.9294070512820513, 0.9327724358974359, 0.9373798076923077, 0.9375400641025641, 0.9376201923076923, 0.9397836538461538, 0.9374198717948717]
[0.24226433967168515, 0.20732168360398365, 0.19516536146402358, 0.1824174696054214, 0.19063646231706327, 0.18508852135676604, 0.18231954402648487, 0.19322129484170522]


Baseline

Test loss: 0.25047120131009953
Test accuracy: 0.9310576923076923
[0.8451923076923077, 0.9231270032051282, 0.9351963141025641, 0.942558092948718, 0.9486378205128205, 0.954296875, 0.9583133012820513, 0.9612980769230769, 0.9647936698717948, 0.9679587339743589, 0.9684395032051282, 0.9721254006410256]
[0.5004368594250618, 0.23124465252726506, 0.18784175401983352, 0.16221658188658647, 0.14232872218753284, 0.12536480555024285, 0.1107771207865041, 0.10138626051947283, 0.0897493811419759, 0.08116478592109604, 0.07677968611988502, 0.06957597367537137]
[0.907051282051282, 0.9225961538461539, 0.9256009615384615, 0.9360977564102564, 0.9282852564102564, 0.935536858974359, 0.9360977564102564, 0.9366185897435897, 0.933974358974359, 0.9327323717948718, 0.9334535256410257, 0.9339342948717949]
[0.2804935275744169, 0.2301636517429963, 0.21692919394908808, 0.195471673592543, 0.2179632740143018, 0.197011992870233, 0.19897643602811374, 0.20249302454101734, 0.22036110651798738, 0.23146647214889526, 0.23198447334460723, 0.23907116112800744]




------------------- Combined
  Using best from Latin dataset
  Test loss: 0.311396842509486
Test accuracy: 0.925842923815896
[0.8667282628485379, 0.937518373861655, 0.9516504976674757, 0.960256464097557, 0.9662515924012711, 0.9718022482631111, 0.9751025436422942]
[0.48752487640907005, 0.20926621026721648, 0.15653001131657565, 0.12441787738736583, 0.10133254733071431, 0.08400507259795782, 0.07188082648027352]
[0.9218848431413942, 0.938081838926821, 0.941175647111548, 0.9469012921207229, 0.9476152478563018, 0.9499531029084541, 0.9478672322324361]
[0.2663467146686018, 0.20403194072363937, 0.19376148110570304, 0.17913857369865285, 0.1796645533240344, 0.18793468398876162, 0.19002771833820664]



  Using best from Hirigana dataset
  
  Test loss: 0.29252755432576616
Test accuracy: 0.9275110789037689
[0.8777840773897052, 0.9462643316114401, 0.9590595383052872, 0.9675535116806069, 0.9732581579908667]
[0.4440669958114477, 0.1780369208709657, 0.12862824064007886, 0.09786757897292178, 0.07775998155507947]
[0.934834040289502, 0.947839233967494, 0.9504150742653955, 0.9511570282642476, 0.9475172539302563]
[0.22164863536907942, 0.17598402402230276, 0.1706780423500132, 0.17109191381201547, 0.18540794638844074]


4 epochs, Kernel_size 5, 700 dims 
Test loss: 0.26691136404697186
Test accuracy: 0.9295499351343325
[0.8745712765791832, 0.9436954908803998, 0.95742163985666, 0.9661640978242659]
[0.4546890144060364, 0.18580689065080871, 0.13555305832085238, 0.10331188303320557]
[0.9307742919947308, 0.945781361556418, 0.9470692817053687, 0.9505270673226605]
[0.23354679287967708, 0.17918825171731526, 0.17388221660271863, 0.16657967375019797]

  
  
precision    recall  f1-score   support

0       0.94      0.95      0.94       800
1       0.93      0.97      0.95       800
2       0.96      0.97      0.96       800
3       0.98      0.90      0.93       800
4       0.91      0.99      0.95       800
5       0.96      0.95      0.96       800
6       0.89      0.79      0.84       800
7       0.93      0.95      0.94       800
8       0.73      0.78      0.75       800
9       0.96      0.94      0.95       800
10       0.96      0.97      0.97       800
11       0.79      0.71      0.75       800
12       0.96      0.98      0.97       800
13       0.94      0.95      0.95       800
14       0.93      0.98      0.95       800
15       0.97      0.99      0.98       800
16       0.84      0.89      0.86       800
17       0.95      0.95      0.95       800
18       0.97      0.98      0.98       800
19       0.94      0.97      0.96       800
20       0.93      0.94      0.93       800
21       0.93      0.93      0.93       800
22       0.99      0.96      0.97       800
23       0.96      0.96      0.96       800
24       0.96      0.95      0.95       800
25       0.95      0.98      0.97       800
26       0.90      0.96      0.93      1000
27       0.99      0.92      0.95      1000
28       0.92      0.96      0.94      1000
29       0.79      0.94      0.86       126
30       0.97      0.94      0.95      1000
31       0.90      0.88      0.89      1000
32       0.94      0.92      0.93      1000
33       0.91      0.91      0.91      1000
34       0.94      0.90      0.92       767
35       0.94      0.93      0.94      1000
36       0.97      0.92      0.95      1000
37       0.92      0.95      0.94      1000
38       0.94      0.91      0.92      1000
39       0.87      0.91      0.89       678
40       0.95      0.85      0.90       629
41       0.91      0.97      0.94      1000
42       0.96      0.97      0.96       418
43       0.94      0.95      0.94      1000
44       0.93      0.93      0.93      1000
45       0.96      0.96      0.96      1000
46       0.93      0.91      0.92      1000
47       0.93      0.94      0.93      1000
48       0.93      0.93      0.93       336
49       0.95      0.93      0.94       399
50       0.92      0.96      0.94      1000
51       0.96      0.89      0.92      1000
52       0.92      0.97      0.94       836
53       0.93      0.93      0.93      1000
54       0.96      0.93      0.95      1000
55       0.87      0.90      0.88       324
56       0.92      0.94      0.93      1000
57       0.97      0.90      0.93       498
58       0.97      0.88      0.92       280
59       0.94      0.93      0.94       552
60       0.96      0.96      0.96      1000
61       0.95      0.95      0.95      1000
62       0.88      0.92      0.90       260
63       0.97      0.97      0.97      1000
64       0.94      0.92      0.93      1000
65       0.90      0.93      0.91      1000
66       0.96      0.88      0.92      1000
67       0.94      0.96      0.95      1000
68       0.92      0.95      0.93       348
69       0.92      0.92      0.92       390
70       0.86      0.82      0.84        68
71       0.90      0.84      0.87        64
72       0.91      0.97      0.94      1000
73       0.96      0.98      0.97      1000
74       0.88      0.86      0.87       574

micro avg       0.93      0.93      0.93     59347
macro avg       0.93      0.93      0.93     59347
weighted avg       0.93      0.93      0.93     59347

Confusion Matrix:    as English     as Japanese
English              20735      65
Japanese              200      38347
Test loss: 0.28493873654119917
Test accuracy: 0.9323975938146698
[0.8822672994257666, 0.9470902804051182, 0.960812929598365, 0.9690059216311899, 0.9745740764055939]
[0.4261978288127763, 0.1743580800648584, 0.12308193711917227, 0.09243601697624175, 0.07324675327785972]
[0.9357299847417805, 0.9443114526899332, 0.948063220080355, 0.9503730768692341, 0.9522069631682836]
[0.21958826651577587, 0.18870649408731233, 0.17614823341311586, 0.17598565366842636, 0.18275323337668073]

Confusion Matrix:    as English     as Japanese
English              20711      89
Japanese              156      38391
Test loss: 0.2834495710498409
Test accuracy: 0.9327345948405142
[0.8813678551954812, 0.946971287780992, 0.9611104111528396, 0.9690024218506798, 0.974528579226419]
[0.4298976942035937, 0.17358968680092146, 0.12201563089062993, 0.09173763517740467, 0.07297981598276164]
[0.933406128821682, 0.9463693251026781, 0.9511290300009744, 0.9490991558531716, 0.9528229249803202]
[0.2245621694972854, 0.18074186627069108, 0.16929511031941882, 0.17968127930016478, 0.1810121853569742]


    
  Accidentally ran with 5000 dims in the Dense layer, going to let complete for 
  my own amusement
    Preformed very poorly

    
  
